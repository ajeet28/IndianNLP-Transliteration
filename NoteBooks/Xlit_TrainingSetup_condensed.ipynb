{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Xlit_TrainingSetup_Condensed.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "-jfVIZzEKHZB",
        "WjNms0lEji1_",
        "fdy2XH9fldK8",
        "2LC0tg-klP9D",
        "-XLm3rKEmVma",
        "0pTRIOhkJ1pY",
        "k81RNgwckP3w",
        "s54j8UNIOHuS",
        "SdO4DI8aXh90"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "-jxbsVmqj2ca"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import json\n",
        "import csv\n",
        "\n",
        "from tqdm import tqdm\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-jfVIZzEKHZB"
      },
      "source": [
        "#Networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gA6LfjkeKLdw"
      },
      "source": [
        "###Encoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7z5hP1qiKKuX"
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "    '''\n",
        "    Simple RNN based encoder network\n",
        "    '''\n",
        "    def __init__(self, input_dim, embed_dim, hidden_dim ,\n",
        "                       rnn_type = 'gru', layers = 1,\n",
        "                       bidirectional =False,\n",
        "                       dropout = 0, device = \"cpu\"):\n",
        "        super(Encoder, self).__init__()\n",
        "\n",
        "        self.input_dim = input_dim #src_vocab_sz\n",
        "        self.enc_embed_dim = embed_dim\n",
        "        self.enc_hidden_dim = hidden_dim\n",
        "        self.enc_rnn_type = rnn_type\n",
        "        self.enc_layers = layers\n",
        "        self.enc_directions = 2 if bidirectional else 1\n",
        "        self.device = device\n",
        "\n",
        "        self.embedding = nn.Embedding(self.input_dim, self.enc_embed_dim)\n",
        "\n",
        "        if self.enc_rnn_type == \"gru\":\n",
        "            self.enc_rnn = nn.GRU(input_size= self.enc_embed_dim,\n",
        "                          hidden_size= self.enc_hidden_dim,\n",
        "                          num_layers= self.enc_layers,\n",
        "                          bidirectional= bidirectional)\n",
        "        elif self.enc_rnn_type == \"lstm\":\n",
        "            self.enc_rnn = nn.LSTM(input_size= self.enc_embed_dim,\n",
        "                          hidden_size= self.enc_hidden_dim,\n",
        "                          num_layers= self.enc_layers,\n",
        "                          bidirectional= bidirectional)\n",
        "        else:\n",
        "            raise Exception(\"unknown RNN type mentioned\")\n",
        "\n",
        "    def forward(self, x, x_sz, hidden = None):\n",
        "        '''\n",
        "        x_sz: (batch_size, 1) -  Unpadded sequence lengths used for pack_pad\n",
        "\n",
        "        Return:\n",
        "            output: (batch_size, max_length, hidden_dim)\n",
        "            hidden: (n_layer*num_directions, batch_size, hidden_dim) | if LSTM tuple -(h_n, c_n)\n",
        "\n",
        "        '''\n",
        "        batch_sz = x.shape[0]\n",
        "        # x: batch_size, max_length, enc_embed_dim\n",
        "        x = self.embedding(x)\n",
        "\n",
        "        ## pack the padded data\n",
        "        # x: max_length, batch_size, enc_embed_dim -> for pack_pad\n",
        "        x = x.permute(1,0,2)\n",
        "        x = nn.utils.rnn.pack_padded_sequence(x, x_sz, enforce_sorted=False) # unpad\n",
        "\n",
        "        # output: packed_size, batch_size, enc_embed_dim --> hidden from all timesteps\n",
        "        # hidden: n_layer**num_directions, batch_size, hidden_dim | if LSTM (h_n, c_n)\n",
        "        output, hidden = self.enc_rnn(x)\n",
        "\n",
        "        ## pad the sequence to the max length in the batch\n",
        "        # output: max_length, batch_size, enc_emb_dim*directions)\n",
        "        output, _ = nn.utils.rnn.pad_packed_sequence(output)\n",
        "\n",
        "        # output: batch_size, max_length, hidden_dim\n",
        "        output = output.permute(1,0,2)\n",
        "\n",
        "        return output, hidden"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N1ZMk_d7KpUf"
      },
      "source": [
        "###Decoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o6toJQq-Kspw"
      },
      "source": [
        "class Decoder(nn.Module):\n",
        "    '''\n",
        "    Used as decoder stage\n",
        "    '''\n",
        "    def __init__(self, output_dim, embed_dim, hidden_dim,\n",
        "                       rnn_type = 'gru', layers = 1,\n",
        "                       use_attention = True,\n",
        "                       enc_outstate_dim = None, # enc_directions * enc_hidden_dim\n",
        "                       dropout = 0, device = \"cpu\"):\n",
        "        super(Decoder, self).__init__()\n",
        "\n",
        "        self.output_dim = output_dim #tgt_vocab_sz\n",
        "        self.dec_hidden_dim = hidden_dim\n",
        "        self.dec_embed_dim = embed_dim\n",
        "        self.dec_rnn_type = rnn_type\n",
        "        self.dec_layers = layers\n",
        "        self.use_attention = use_attention\n",
        "        self.device = device\n",
        "        if self.use_attention:\n",
        "            self.enc_outstate_dim = enc_outstate_dim if enc_outstate_dim else hidden_dim\n",
        "        else:\n",
        "            self.enc_outstate_dim = 0\n",
        "\n",
        "\n",
        "        self.embedding = nn.Embedding(self.output_dim, self.dec_embed_dim)\n",
        "\n",
        "        if self.dec_rnn_type == 'gru':\n",
        "            self.dec_rnn = nn.GRU(input_size= self.dec_embed_dim + self.enc_outstate_dim, # to concat attention_output\n",
        "                          hidden_size= self.dec_hidden_dim, # previous Hidden\n",
        "                          num_layers= self.dec_layers,\n",
        "                          batch_first = True )\n",
        "        elif self.dec_rnn_type == \"lstm\":\n",
        "            self.dec_rnn = nn.LSTM(input_size= self.dec_embed_dim + self.enc_outstate_dim, # to concat attention_output\n",
        "                          hidden_size= self.dec_hidden_dim, # previous Hidden\n",
        "                          num_layers= self.dec_layers,\n",
        "                          batch_first = True )\n",
        "        else:\n",
        "            raise Exception(\"unknown RNN type mentioned\")\n",
        "\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(self.dec_hidden_dim, self.dec_embed_dim), nn.LeakyReLU(),\n",
        "            # nn.Linear(self.dec_embed_dim, self.dec_embed_dim), nn.LeakyReLU(), # removing to reduce size\n",
        "            nn.Linear(self.dec_embed_dim, self.output_dim),\n",
        "            )\n",
        "\n",
        "        ##----- Attention ----------\n",
        "        if self.use_attention:\n",
        "            self.W1 = nn.Linear( self.enc_outstate_dim, self.dec_hidden_dim)\n",
        "            self.W2 = nn.Linear( self.dec_hidden_dim, self.dec_hidden_dim)\n",
        "            self.V = nn.Linear( self.dec_hidden_dim, 1)\n",
        "\n",
        "    def attention(self, x, hidden, enc_output):\n",
        "        '''\n",
        "        x: (batch_size, 1, dec_embed_dim) -> after Embedding\n",
        "        enc_output: batch_size, max_length, enc_hidden_dim *num_directions\n",
        "        hidden: n_layers, batch_size, hidden_size | if LSTM (h_n, c_n)\n",
        "        '''\n",
        "\n",
        "        ## perform addition to calculate the score\n",
        "\n",
        "        # hidden_with_time_axis: batch_size, 1, hidden_dim\n",
        "        ## hidden_with_time_axis = hidden.permute(1, 0, 2) ## replaced with below 2lines\n",
        "        hidden_with_time_axis = torch.sum(hidden, axis=0)\n",
        "\n",
        "        hidden_with_time_axis = hidden_with_time_axis.unsqueeze(1)\n",
        "\n",
        "        # score: batch_size, max_length, hidden_dim\n",
        "        score = torch.tanh(self.W1(enc_output) + self.W2(hidden_with_time_axis))\n",
        "\n",
        "        # attention_weights: batch_size, max_length, 1\n",
        "        # we get 1 at the last axis because we are applying score to self.V\n",
        "        attention_weights = torch.softmax(self.V(score), dim=1)\n",
        "\n",
        "        # context_vector shape after sum == (batch_size, hidden_dim)\n",
        "        context_vector = attention_weights * enc_output\n",
        "        context_vector = torch.sum(context_vector, dim=1)\n",
        "        # context_vector: batch_size, 1, hidden_dim\n",
        "        context_vector = context_vector.unsqueeze(1)\n",
        "\n",
        "        # attend_out (batch_size, 1, dec_embed_dim + hidden_size)\n",
        "        attend_out = torch.cat((context_vector, x), -1)\n",
        "\n",
        "        return attend_out, attention_weights\n",
        "\n",
        "    def forward(self, x, hidden, enc_output):\n",
        "        '''\n",
        "        x: (batch_size, 1)\n",
        "        enc_output: batch_size, max_length, dec_embed_dim\n",
        "        hidden: n_layer, batch_size, hidden_size | lstm: (h_n, c_n)\n",
        "        '''\n",
        "        if (hidden is None) and (self.use_attention is False):\n",
        "            raise Exception( \"No use of a decoder with No attention and No Hidden\")\n",
        "\n",
        "        batch_sz = x.shape[0]\n",
        "\n",
        "        if hidden is None:\n",
        "            # hidden: n_layers, batch_size, hidden_dim\n",
        "            hid_for_att = torch.zeros((self.dec_layers, batch_sz,\n",
        "                                    self.dec_hidden_dim )).to(self.device)\n",
        "        elif self.dec_rnn_type == 'lstm':\n",
        "            hid_for_att = hidden[0] # h_n\n",
        "        else:\n",
        "            hid_for_att = hidden\n",
        "\n",
        "        # x (batch_size, 1, dec_embed_dim) -> after embedding\n",
        "        x = self.embedding(x)\n",
        "\n",
        "        if self.use_attention:\n",
        "            # x (batch_size, 1, dec_embed_dim + hidden_size) -> after attention\n",
        "            # aw: (batch_size, max_length, 1)\n",
        "            x, aw = self.attention( x, hid_for_att, enc_output)\n",
        "        else:\n",
        "            x, aw = x, 0\n",
        "\n",
        "        # passing the concatenated vector to the GRU\n",
        "        # output: (batch_size, n_layers, hidden_size)\n",
        "        # hidden: n_layers, batch_size, hidden_size | if LSTM (h_n, c_n)\n",
        "        output, hidden = self.dec_rnn(x, hidden) if hidden is not None else self.dec_rnn(x)\n",
        "\n",
        "        # output :shp: (batch_size * 1, hidden_size)\n",
        "        output =  output.view(-1, output.size(2))\n",
        "\n",
        "        # output :shp: (batch_size * 1, output_dim)\n",
        "        output = self.fc(output)\n",
        "\n",
        "        return output, hidden, aw"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fOlVUXqrK1eb"
      },
      "source": [
        "### Seq2Seq Connection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vPaXxUy3LGz3"
      },
      "source": [
        "class Seq2Seq(nn.Module):\n",
        "    '''\n",
        "    Used to construct seq2seq architecture with encoder decoder objects\n",
        "    '''\n",
        "    def __init__(self, encoder, decoder, pass_enc2dec_hid=False, dropout = 0, device = \"cpu\"):\n",
        "        super(Seq2Seq, self).__init__()\n",
        "\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.device = device\n",
        "        self.pass_enc2dec_hid = pass_enc2dec_hid\n",
        "\n",
        "        if self.pass_enc2dec_hid:\n",
        "            assert decoder.dec_hidden_dim == encoder.enc_hidden_dim, \"Hidden Dimension of encoder and decoder must be same, or unset `pass_enc2dec_hid`\"\n",
        "        if decoder.use_attention:\n",
        "            assert decoder.enc_outstate_dim == encoder.enc_directions*encoder.enc_hidden_dim,\"Set `enc_out_dim` correctly in decoder\"\n",
        "        assert self.pass_enc2dec_hid or decoder.use_attention, \"No use of a decoder with No attention and No Hidden from Encoder\"\n",
        "\n",
        "\n",
        "    def forward(self, src, tgt, src_sz, teacher_forcing_ratio = 0):\n",
        "        '''\n",
        "        src: (batch_size, sequence_len.padded)\n",
        "        tgt: (batch_size, sequence_len.padded)\n",
        "        src_sz: [batch_size, 1] -  Unpadded sequence lengths\n",
        "        '''\n",
        "        batch_size = tgt.shape[0]\n",
        "\n",
        "        # enc_output: (batch_size, padded_seq_length, enc_hidden_dim*num_direction)\n",
        "        # enc_hidden: (enc_layers*num_direction, batch_size, hidden_dim)\n",
        "        enc_output, enc_hidden = self.encoder(src, src_sz)\n",
        "\n",
        "        if self.pass_enc2dec_hid:\n",
        "           # dec_hidden: dec_layers, batch_size , dec_hidden_dim\n",
        "            dec_hidden = enc_hidden\n",
        "        else:\n",
        "            # dec_hidden -> Will be initialized to zeros internally\n",
        "            dec_hidden = None\n",
        "\n",
        "        # pred_vecs: (batch_size, output_dim, sequence_sz) -> shape required for CELoss\n",
        "        pred_vecs = torch.zeros(batch_size, self.decoder.output_dim, tgt.size(1)).to(self.device)\n",
        "\n",
        "        # dec_input: (batch_size, 1)\n",
        "        dec_input = tgt[:,0].unsqueeze(1) # initialize to start token\n",
        "        pred_vecs[:,1,0] = 1 # Initialize to start tokens all batches\n",
        "        for t in range(1, tgt.size(1)):\n",
        "            # dec_hidden: dec_layers, batch_size , dec_hidden_dim\n",
        "            # dec_output: batch_size, output_dim\n",
        "            # dec_input: (batch_size, 1)\n",
        "            dec_output, dec_hidden, _ = self.decoder( dec_input,\n",
        "                                               dec_hidden,\n",
        "                                               enc_output,  )\n",
        "            pred_vecs[:,:,t] = dec_output\n",
        "\n",
        "            # # prediction: batch_size\n",
        "            prediction = torch.argmax(dec_output, dim=1)\n",
        "\n",
        "            # Teacher Forcing\n",
        "            if random.random() < teacher_forcing_ratio:\n",
        "                dec_input = tgt[:, t].unsqueeze(1)\n",
        "            else:\n",
        "                dec_input = prediction.unsqueeze(1)\n",
        "\n",
        "        return pred_vecs #(batch_size, output_dim, sequence_sz)\n",
        "\n",
        "    def inference(self, src, max_tgt_sz=50, debug = 0):\n",
        "        '''\n",
        "        single input only, No batch Inferencing\n",
        "        src: (sequence_len)\n",
        "        debug: if True will return attention weights also\n",
        "        '''\n",
        "        batch_size = 1\n",
        "        start_tok = src[0]\n",
        "        end_tok = src[-1]\n",
        "        src_sz = torch.tensor([len(src)])\n",
        "        src_ = src.unsqueeze(0)\n",
        "\n",
        "        # enc_output: (batch_size, padded_seq_length, enc_hidden_dim*num_direction)\n",
        "        # enc_hidden: (enc_layers*num_direction, batch_size, hidden_dim)\n",
        "        enc_output, enc_hidden = self.encoder(src_, src_sz)\n",
        "\n",
        "        if self.pass_enc2dec_hid:\n",
        "            # dec_hidden: dec_layers, batch_size , dec_hidden_dim\n",
        "            dec_hidden = enc_hidden\n",
        "        else:\n",
        "            # dec_hidden -> Will be initialized to zeros internally\n",
        "            dec_hidden = None\n",
        "\n",
        "        # pred_arr: (sequence_sz, 1) -> shape required for CELoss\n",
        "        pred_arr = torch.zeros(max_tgt_sz, 1).to(self.device)\n",
        "        if debug: attend_weight_arr = torch.zeros(max_tgt_sz, len(src)).to(self.device)\n",
        "\n",
        "        # dec_input: (batch_size, 1)\n",
        "        dec_input = start_tok.view(1,1) # initialize to start token\n",
        "        pred_arr[0] = start_tok.view(1,1) # initialize to start token\n",
        "        for t in range(max_tgt_sz):\n",
        "            # dec_hidden: dec_layers, batch_size , dec_hidden_dim\n",
        "            # dec_output: batch_size, output_dim\n",
        "            # dec_input: (batch_size, 1)\n",
        "            dec_output, dec_hidden, aw = self.decoder( dec_input,\n",
        "                                               dec_hidden,\n",
        "                                               enc_output,  )\n",
        "            # prediction :shp: (1,1)\n",
        "            prediction = torch.argmax(dec_output, dim=1)\n",
        "            dec_input = prediction.unsqueeze(1)\n",
        "            pred_arr[t] = prediction\n",
        "            if debug: attend_weight_arr[t] = aw.squeeze(-1)\n",
        "\n",
        "            if torch.eq(prediction, end_tok):\n",
        "                break\n",
        "\n",
        "        if debug: return pred_arr.squeeze(), attend_weight_arr\n",
        "        # pred_arr :shp: (sequence_len)\n",
        "        return pred_arr.squeeze().to(dtype=torch.long)\n",
        "\n",
        "\n",
        "    def active_beam_inference(self, src, beam_width=3, max_tgt_sz=50):\n",
        "        ''' Active beam Search based decoding\n",
        "        src: (sequence_len)\n",
        "        '''\n",
        "        def _avg_score(p_tup):\n",
        "            ''' Used for Sorting\n",
        "            TODO: Dividing by length of sequence power alpha as hyperparam\n",
        "            '''\n",
        "            return p_tup[0]\n",
        "\n",
        "        batch_size = 1\n",
        "        start_tok = src[0]\n",
        "        end_tok = src[-1]\n",
        "        src_sz = torch.tensor([len(src)])\n",
        "        src_ = src.unsqueeze(0)\n",
        "\n",
        "        # enc_output: (batch_size, padded_seq_length, enc_hidden_dim*num_direction)\n",
        "        # enc_hidden: (enc_layers*num_direction, batch_size, hidden_dim)\n",
        "        enc_output, enc_hidden = self.encoder(src_, src_sz)\n",
        "\n",
        "        if self.pass_enc2dec_hid:\n",
        "            # dec_hidden: dec_layers, batch_size , dec_hidden_dim\n",
        "            init_dec_hidden = enc_hidden\n",
        "        else:\n",
        "            # dec_hidden -> Will be initialized to zeros internally\n",
        "            init_dec_hidden = None\n",
        "\n",
        "        # top_pred[][0] = Σ-log_softmax\n",
        "        # top_pred[][1] = sequence torch.tensor shape: (1)\n",
        "        # top_pred[][2] = dec_hidden\n",
        "        top_pred_list = [ (0, start_tok.unsqueeze(0) , init_dec_hidden) ]\n",
        "\n",
        "        for t in range(max_tgt_sz):\n",
        "            cur_pred_list = []\n",
        "\n",
        "            for p_tup in top_pred_list:\n",
        "                if p_tup[1][-1] == end_tok:\n",
        "                    cur_pred_list.append(p_tup)\n",
        "                    continue\n",
        "\n",
        "                # dec_hidden: dec_layers, 1, hidden_dim\n",
        "                # dec_output: 1, output_dim\n",
        "                dec_output, dec_hidden, _ = self.decoder( x = p_tup[1][-1].view(1,1), #dec_input: (1,1)\n",
        "                                                    hidden = p_tup[2],\n",
        "                                                    enc_output = enc_output, )\n",
        "\n",
        "                ## π{prob} = Σ{log(prob)} -> to prevent diminishing\n",
        "                # dec_output: (1, output_dim)\n",
        "                dec_output = nn.functional.log_softmax(dec_output, dim=1)\n",
        "                # pred_topk.values & pred_topk.indices: (1, beam_width)\n",
        "                pred_topk = torch.topk(dec_output, k=beam_width, dim=1)\n",
        "\n",
        "                for i in range(beam_width):\n",
        "                    sig_logsmx_ = p_tup[0] + pred_topk.values[0][i]\n",
        "                    # seq_tensor_ : (seq_len)\n",
        "                    seq_tensor_ = torch.cat( (p_tup[1], pred_topk.indices[0][i].view(1)) )\n",
        "\n",
        "                    cur_pred_list.append( (sig_logsmx_, seq_tensor_, dec_hidden) )\n",
        "\n",
        "            cur_pred_list.sort(key = _avg_score, reverse =True) # Maximized order\n",
        "            top_pred_list = cur_pred_list[:beam_width]\n",
        "\n",
        "            # check if end_tok of all topk\n",
        "            end_flags_ = [1 if t[1][-1] == end_tok else 0 for t in top_pred_list]\n",
        "            if beam_width == sum( end_flags_ ): break\n",
        "\n",
        "        pred_tnsr_list = [t[1] for t in top_pred_list ]\n",
        "\n",
        "        return pred_tnsr_list\n",
        "\n",
        "    def passive_beam_inference(self, src, beam_width = 7, max_tgt_sz=50):\n",
        "        '''\n",
        "        Passive Beam search based inference\n",
        "        src: (sequence_len)\n",
        "        '''\n",
        "        def _avg_score(p_tup):\n",
        "            ''' Used for Sorting\n",
        "            TODO: Dividing by length of sequence power alpha as hyperparam\n",
        "            '''\n",
        "            return  p_tup[0]\n",
        "\n",
        "        def _beam_search_topk(topk_obj, start_tok, beam_width):\n",
        "            ''' search for sequence with maxim prob\n",
        "            topk_obj[x]: .values & .indices shape:(1, beam_width)\n",
        "            '''\n",
        "            # top_pred_list[x]: tuple(prob, seq_tensor)\n",
        "            top_pred_list = [ (0, start_tok.unsqueeze(0) ), ]\n",
        "\n",
        "            for obj in topk_obj:\n",
        "                new_lst_ = list()\n",
        "                for itm in top_pred_list:\n",
        "                    for i in range(beam_width):\n",
        "                        sig_logsmx_ = itm[0] + obj.values[0][i]\n",
        "                        seq_tensor_ = torch.cat( (itm[1] , obj.indices[0][i].view(1) ) )\n",
        "                        new_lst_.append( (sig_logsmx_, seq_tensor_) )\n",
        "\n",
        "                new_lst_.sort(key = _avg_score, reverse =True)\n",
        "                top_pred_list = new_lst_[:beam_width]\n",
        "            return top_pred_list\n",
        "\n",
        "        batch_size = 1\n",
        "        start_tok = src[0]\n",
        "        end_tok = src[-1]\n",
        "        src_sz = torch.tensor([len(src)])\n",
        "        src_ = src.unsqueeze(0)\n",
        "\n",
        "        enc_output, enc_hidden = self.encoder(src_, src_sz)\n",
        "\n",
        "        if self.pass_enc2dec_hid:\n",
        "            # dec_hidden: dec_layers, batch_size , dec_hidden_dim\n",
        "            dec_hidden = enc_hidden\n",
        "        else:\n",
        "            # dec_hidden -> Will be initialized to zeros internally\n",
        "            dec_hidden = None\n",
        "\n",
        "        # dec_input: (1, 1)\n",
        "        dec_input = start_tok.view(1,1) # initialize to start token\n",
        "\n",
        "\n",
        "        topk_obj = []\n",
        "        for t in range(max_tgt_sz):\n",
        "            dec_output, dec_hidden, aw = self.decoder( dec_input,\n",
        "                                               dec_hidden,\n",
        "                                               enc_output,  )\n",
        "\n",
        "            ## π{prob} = Σ{log(prob)} -> to prevent diminishing\n",
        "            # dec_output: (1, output_dim)\n",
        "            dec_output = nn.functional.log_softmax(dec_output, dim=1)\n",
        "            # pred_topk.values & pred_topk.indices: (1, beam_width)\n",
        "            pred_topk = torch.topk(dec_output, k=beam_width, dim=1)\n",
        "\n",
        "            topk_obj.append(pred_topk)\n",
        "\n",
        "            # dec_input: (1, 1)\n",
        "            dec_input = pred_topk.indices[0][0].view(1,1)\n",
        "            if torch.eq(dec_input, end_tok):\n",
        "                break\n",
        "\n",
        "        top_pred_list = _beam_search_topk(topk_obj, start_tok, beam_width)\n",
        "        pred_tnsr_list = [t[1] for t in top_pred_list ]\n",
        "\n",
        "        return pred_tnsr_list\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WjNms0lEji1_"
      },
      "source": [
        "#Data Handling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fdy2XH9fldK8"
      },
      "source": [
        "### Unicodes\n",
        "\n",
        "Add necessary Unicodes for specific script(langauge) below as a list. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6pigemVDle0z"
      },
      "source": [
        "indoarab_num = [chr(alpha) for alpha in range(48, 58)]\n",
        "\n",
        "english_lower_script = [chr(alpha) for alpha in range(97, 123)]\n",
        "\n",
        "devanagari_script = ['ऄ', 'अ', 'आ', 'इ', 'ई', 'उ', 'ऊ','ऍ', 'ऎ', 'ए', 'ऐ',\n",
        "    'ऑ', 'ऒ', 'ओ', 'औ','ऋ','ॠ','ऌ','ॡ','ॲ', 'ॐ',\n",
        "    'क', 'ख', 'ग', 'घ', 'ङ', 'च', 'छ', 'ज', 'झ', 'ञ', 'ट', 'ठ', 'ड', 'ढ', 'ण',\n",
        "    'त', 'थ', 'द', 'ध', 'न', 'ऩ', 'प', 'फ', 'ब', 'भ', 'म', 'य', 'र', 'ऱ', 'ल',\n",
        "    'ळ', 'ऴ', 'व', 'श', 'ष', 'स', 'ह', 'क़', 'ख़', 'ग़', 'ज़', 'ड़', 'ढ़', 'फ़', 'य़',\n",
        "    '्', 'ा', 'ि', 'ी', 'ु', 'ू', 'ॅ', 'ॆ', 'े', 'ै', 'ॉ', 'ॊ', 'ो', 'ौ',\n",
        "    'ृ', 'ॄ', 'ॢ', 'ॣ', 'ँ', 'ं', 'ः', '़', '॑',  'ऽ',\n",
        "\n",
        "    chr(0x200c), # ZeroWidth-NonJoiner U+200c\n",
        "    chr(0x200d), # ZeroWidthJoiner U+200d\n",
        "]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2LC0tg-klP9D"
      },
      "source": [
        "### Glyph handler"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MWXKBNkWjMhk"
      },
      "source": [
        "class GlyphStrawboss():\n",
        "    def __init__(self, lang_script = english_lower_script):\n",
        "        \"\"\" list of letters in a language in unicode\n",
        "        lang: List with unicodes\n",
        "        \"\"\"\n",
        "        self.glyphs = lang_script\n",
        "\n",
        "        self.char2idx = {}\n",
        "        self.idx2char = {}\n",
        "        self._create_index()\n",
        "\n",
        "    def _create_index(self):\n",
        "\n",
        "        self.char2idx['_'] = 0  #pad\n",
        "        self.char2idx['$'] = 1  #start\n",
        "        self.char2idx['#'] = 2  #end\n",
        "        self.char2idx['*'] = 3  #Mask\n",
        "        self.char2idx[\"'\"] = 4  #apostrophe U+0027\n",
        "        self.char2idx['%'] = 5  #unused\n",
        "        self.char2idx['!'] = 6  #unused\n",
        "\n",
        "        # letter to index mapping\n",
        "        for idx, char in enumerate(self.glyphs):\n",
        "            self.char2idx[char] = idx + 7 # +7 token initially\n",
        "\n",
        "        # index to letter mapping\n",
        "        for char, idx in self.char2idx.items():\n",
        "            self.idx2char[idx] = char\n",
        "\n",
        "    def size(self):\n",
        "        return len(self.char2idx)\n",
        "\n",
        "\n",
        "    def word2xlitvec(self, word):\n",
        "        \"\"\" Converts given string of gyphs(word) to vector(numpy)\n",
        "        Also adds tokens for start and end\n",
        "        \"\"\"\n",
        "        try:\n",
        "            vec = [self.char2idx['$']] #start token\n",
        "            for i in list(word):\n",
        "                vec.append(self.char2idx[i])\n",
        "            vec.append(self.char2idx['#']) #end token\n",
        "\n",
        "            vec = np.asarray(vec, dtype=np.int64)\n",
        "            return vec\n",
        "\n",
        "        except Exception as error:\n",
        "            print(\"Error In word:\", word, \"Error Char not in Token:\", error)\n",
        "            sys.exit()\n",
        "\n",
        "    def xlitvec2word(self, vector):\n",
        "        \"\"\" Converts vector(numpy) to string of glyphs(word)\n",
        "        \"\"\"\n",
        "        char_list = []\n",
        "        for i in vector:\n",
        "            char_list.append(self.idx2char[i])\n",
        "\n",
        "        word = \"\".join(char_list).replace('$','').replace('#','') # remove tokens\n",
        "        word = word.replace(\"_\", \"\").replace('*','') # remove tokens\n",
        "        return word\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-XLm3rKEmVma"
      },
      "source": [
        "### Dataset Class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9g52_kqlmd_5"
      },
      "source": [
        "from torch.utils.data import Dataset\n",
        "\n",
        "class XlitData(Dataset):\n",
        "    \"\"\" Backtransliteration from English to Native Language\n",
        "    JSON format only\n",
        "    depends on: Numpy\n",
        "    \"\"\"\n",
        "    def __init__(self, src_glyph_obj, tgt_glyph_obj,\n",
        "                    json_file, file_map = \"LangEn\",\n",
        "                    padding = True, max_seq_size = None,\n",
        "                 ):\n",
        "        \"\"\"\n",
        "        padding: Set True if Padding with zeros is required for Batching\n",
        "        max_seq_size: Size for Padding both input and output, Longer words will be truncated\n",
        "                      If unset computes maximum of source, target seperate\n",
        "        \"\"\"\n",
        "        #Load data\n",
        "        if file_map == \"LangEn\": # output-input\n",
        "            tgt_str, src_str = self._json2_k_v(json_file)\n",
        "        elif file_map == \"EnLang\": # input-output\n",
        "            src_str, tgt_str = self._json2_k_v(json_file)\n",
        "        else:\n",
        "            raise Exception('Unknown JSON structure')\n",
        "\n",
        "        self.src_glyph = src_glyph_obj\n",
        "        self.tgt_glyph = tgt_glyph_obj\n",
        "\n",
        "        __svec = self.src_glyph.word2xlitvec\n",
        "        __tvec = self.tgt_glyph.word2xlitvec\n",
        "        self.src = [ __svec(s)  for s in src_str]\n",
        "        self.tgt = [ __tvec(s)  for s in tgt_str]\n",
        "\n",
        "        self.tgt_class_weights = self._char_class_weights(self.tgt)\n",
        "\n",
        "        self.padding = padding\n",
        "        if max_seq_size:\n",
        "            self.max_tgt_size = max_seq_size\n",
        "            self.max_src_size = max_seq_size\n",
        "        else:\n",
        "            self.max_src_size = max(len(t) for t in self.src)\n",
        "            self.max_tgt_size = max(len(t) for t in self.tgt)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        x_sz = len(self.src[index])\n",
        "        y_sz = len(self.tgt[index])\n",
        "        if self.padding:\n",
        "            x = self._pad_sequence(self.src[index], self.max_src_size)\n",
        "            y = self._pad_sequence(self.tgt[index], self.max_tgt_size)\n",
        "        else:\n",
        "            x = self.src[index]\n",
        "            y = self.tgt[index]\n",
        "        return x,y, x_sz\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.src)\n",
        "\n",
        "\n",
        "    def _json2_k_v(self, json_file):\n",
        "        ''' Convert JSON lang pairs to Key-Value lists with indexwise one2one correspondance\n",
        "        '''\n",
        "        with open(json_file, 'r', encoding = \"utf-8\") as f:\n",
        "            data = json.load(f)\n",
        "\n",
        "        x = []; y = []\n",
        "        for k in data:\n",
        "            for v in data[k]:\n",
        "                x.append(k); y.append(v)\n",
        "\n",
        "        return x, y\n",
        "\n",
        "\n",
        "    def _pad_sequence(self, x, max_len):\n",
        "        \"\"\" Pad sequence to maximum length;\n",
        "        Pads zero if word < max\n",
        "        Clip word if word > max\n",
        "        \"\"\"\n",
        "        padded = np.zeros((max_len), dtype=np.int64)\n",
        "        if len(x) > max_len: padded[:] = x[:max_len]\n",
        "        else: padded[:len(x)] = x\n",
        "        return padded\n",
        "\n",
        "    def _char_class_weights(self, x_list, scale = 10):\n",
        "        \"\"\"For handling class imbalance in the characters\n",
        "        Return: 1D-tensor will be fed to CEloss weights for error calculation\n",
        "        \"\"\"\n",
        "        from collections import Counter\n",
        "        full_list = []\n",
        "        for x in x_list:\n",
        "            full_list += list(x)\n",
        "        count_dict = dict(Counter(full_list))\n",
        "\n",
        "        class_weights = np.ones(self.tgt_glyph.size(), dtype = np.float32)\n",
        "        for k in count_dict:\n",
        "            class_weights[k] = (1/count_dict[k]) * scale\n",
        "\n",
        "        return class_weights\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0pTRIOhkJ1pY"
      },
      "source": [
        "### Merge JSON"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f-OncNgRkPNl"
      },
      "source": [
        "def merge_xlit_jsons(filepath_list, save_prefix = \"\"):\n",
        "    \"\"\"\n",
        "    Merge JSON files into single file wrt keys \n",
        "    \"\"\"\n",
        "    data_list = []\n",
        "    for fpath in filepath_list:\n",
        "        with open(fpath, 'r', encoding = \"utf-8\") as f:\n",
        "            data_list.append(json.load(f))\n",
        "\n",
        "    whole_dict = dict()\n",
        "    for dat in data_list:\n",
        "        for dk in dat:\n",
        "            whole_dict[dk] = set()\n",
        "\n",
        "    for dat in data_list:\n",
        "        for dk in dat:\n",
        "            whole_dict[dk].update(dat[dk])\n",
        "\n",
        "    for k in whole_dict:\n",
        "        whole_dict[k] = list(whole_dict[k])\n",
        "\n",
        "    print(\"Total Key count:\", len(whole_dict))\n",
        "    save_path = save_prefix+\"merged_file.json\"\n",
        "    with open(save_path,\"w\", encoding = \"utf-8\") as f:\n",
        "        json.dump(whole_dict, f, ensure_ascii=False, indent=4, sort_keys=True,)\n",
        "\n",
        "    return save_path"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k81RNgwckP3w"
      },
      "source": [
        "#Utilities"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-CFrFoBwnbxu"
      },
      "source": [
        "def LOG2CSV(data, csv_file, flag = 'a'):\n",
        "    '''\n",
        "    data: List of elements to be written\n",
        "    '''\n",
        "    with open(csv_file, flag) as csvFile:\n",
        "        writer = csv.writer(csvFile)\n",
        "        writer.writerow(data)\n",
        "    csvFile.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1lFtPQyzJe2U"
      },
      "source": [
        "### Weights related utils"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kK0edtDzIkTx"
      },
      "source": [
        "def load_pretrained(model, weight_path, flexible = False):\n",
        "    if not weight_path:\n",
        "        return model\n",
        "\n",
        "    pretrain_dict = torch.load(weight_path)\n",
        "    model_dict = model.state_dict()\n",
        "    if flexible:\n",
        "        pretrain_dict = {k: v for k, v in pretrain_dict.items() if k in model_dict}\n",
        "    print(\"Pretrained layers:\", pretrain_dict.keys())\n",
        "    model_dict.update(pretrain_dict)\n",
        "    model.load_state_dict(model_dict)\n",
        "\n",
        "    return model\n",
        "\n",
        "def count_train_param(model):\n",
        "    train_params_count = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    print('The model has {} trainable parameters'.format(train_params_count))\n",
        "    return train_params_count\n",
        "\n",
        "def freeze_params(model, exclusion_list = []):\n",
        "    ## TODO: Exclusion lists\n",
        "    for param in model.parameters():\n",
        "        param.requires_grad = False\n",
        "    return model\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VaNeFZknJlzL"
      },
      "source": [
        "### Accuracy Estimation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lr_0a0MfJlgA"
      },
      "source": [
        "def accuracy_score(pred_tnsr, tgt_tnsr, glyph_obj):\n",
        "    '''Simple accuracy calculation for char2char seq TRAINING phase\n",
        "    pred_tnsr: torch tensor :shp: (batch, voc_size, seq_len)\n",
        "    tgt_tnsr: torch tensor :shp: (batch, seq_len)\n",
        "    '''\n",
        "    pred_seq = torch.argmax(pred_tnsr, dim=1)\n",
        "    batch_sz = pred_seq.shape[0]\n",
        "    crt_cnt = 0\n",
        "    for i in range(batch_sz):\n",
        "        pred = glyph_obj.xlitvec2word(pred_seq[i,:].cpu().numpy())\n",
        "        tgt = glyph_obj.xlitvec2word(tgt_tnsr[i,:].cpu().numpy())\n",
        "        if pred == tgt:\n",
        "            crt_cnt += 1\n",
        "    return torch.tensor(crt_cnt/batch_sz)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yOHOVbdoJ-ey"
      },
      "source": [
        "#Training Stage"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B6cTnqVTNRJl"
      },
      "source": [
        "##Configuration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mImpycFFFujt"
      },
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "INST_NAME = \"Training_1\"\n",
        "LOG_PATH = INST_NAME + \"/\" \n",
        "WGT_PREFIX = LOG_PATH+\"weights/\"+INST_NAME\n",
        "if not os.path.exists(LOG_PATH+\"weights\"): os.makedirs(LOG_PATH+\"weights\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wLQwZi4UOuEw"
      },
      "source": [
        "Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RpXHxQU2J-J8"
      },
      "source": [
        "num_epochs = 10\n",
        "batch_size = 1  # Remember to run data objects creation on changing this\n",
        "acc_grad = 1\n",
        "learning_rate = 1e-3\n",
        "teacher_forcing, teach_force_till, teach_decay_pereph = 1, 20, 0\n",
        "pretrain_wgt_path = None\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i5yexIStOzmq"
      },
      "source": [
        "Datasets & Dataloaders"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "noEImVcnO5J8"
      },
      "source": [
        "# from `Datahandling` section\n",
        "src_glyph = GlyphStrawboss(english_lower_script) \n",
        "tgt_glyph = GlyphStrawboss(devanagari_script)\n",
        "\n",
        "TRAIN_FILE = \"checkup-train.json\"\n",
        "VALID_FILE = \"checkup-valid.json\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hq2AO5feQADQ"
      },
      "source": [
        "Network Architecture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UVP7DQrkQDHh"
      },
      "source": [
        "input_dim = src_glyph.size()\n",
        "output_dim = tgt_glyph.size()\n",
        "enc_emb_dim = 300\n",
        "dec_emb_dim = 300\n",
        "enc_hidden_dim = 512\n",
        "dec_hidden_dim = 512\n",
        "rnn_type = \"lstm\"\n",
        "enc2dec_hid = True\n",
        "attention = True\n",
        "enc_layers = 1\n",
        "dec_layers = 2\n",
        "m_dropout = 0\n",
        "enc_bidirect = True\n",
        "enc_outstate_dim = enc_hidden_dim * (2 if enc_bidirect else 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s54j8UNIOHuS"
      },
      "source": [
        "### Instantiation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WFqWiSX3QfAo"
      },
      "source": [
        "Dataset objects creation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-3R61VwpOP4o"
      },
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# train_file = merge_xlit_jsons([\"data/hindi/HiEn_train1.json\",\n",
        "#                                 \"data/hindi/HiEn_train2.json\" ],\n",
        "#                                 save_prefix= LOG_PATH)\n",
        "\n",
        "train_dataset = XlitData( src_glyph_obj = src_glyph, tgt_glyph_obj = tgt_glyph,\n",
        "                        json_file=TRAIN_FILE, file_map = \"LangEn\",\n",
        "                        padding=True)\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size,\n",
        "                                shuffle=True, num_workers=0)\n",
        "\n",
        "val_dataset = XlitData( src_glyph_obj = src_glyph, tgt_glyph_obj = tgt_glyph,\n",
        "                        json_file= VALID_FILE, file_map = \"LangEn\",\n",
        "                        padding=True)\n",
        "\n",
        "\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=batch_size,\n",
        "                                shuffle=True, num_workers=0)\n",
        "\n",
        "# for i in range(len(train_dataset)):\n",
        "#     print(train_dataset.__getitem__(i))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kwym4kg6QlUi"
      },
      "source": [
        "Network Models creation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VCIgqaq3Q5mN"
      },
      "source": [
        "enc = Encoder(  input_dim= input_dim, embed_dim = enc_emb_dim,\n",
        "                hidden_dim= enc_hidden_dim,\n",
        "                rnn_type = rnn_type, layers= enc_layers,\n",
        "                dropout= m_dropout, device = device,\n",
        "                bidirectional= enc_bidirect)\n",
        "dec = Decoder(  output_dim= output_dim, embed_dim = dec_emb_dim,\n",
        "                hidden_dim= dec_hidden_dim,\n",
        "                rnn_type = rnn_type, layers= dec_layers,\n",
        "                dropout= m_dropout,\n",
        "                use_attention = attention,\n",
        "                enc_outstate_dim= enc_outstate_dim,\n",
        "                device = device,)\n",
        "\n",
        "model = Seq2Seq(enc, dec, pass_enc2dec_hid=enc2dec_hid,\n",
        "                device=device)\n",
        "model = model.to(device)\n",
        "\n",
        "model = load_pretrained(model,pretrain_wgt_path) #if path empty returns unmodified\n",
        "\n",
        "## ----- Load Embeds -----\n",
        "### For Loading charecter embedding from pretrained fasttext model \n",
        "\n",
        "# hi_emb_vecs = np.load(\"hi_char_fasttext.npy\") \n",
        "# model.decoder.embedding.weight.data.copy_(torch.from_numpy(hi_emb_vecs))\n",
        "\n",
        "# en_emb_vecs = np.load(\"en_char_fasttext.npy\") \n",
        "# model.encoder.embedding.weight.data.copy_(torch.from_numpy(en_emb_vecs))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1UPZ3K_sRgTI"
      },
      "source": [
        "count_train_param(model)\n",
        "print(model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sOcyOudmEAdB"
      },
      "source": [
        "## Optimization Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Isq0bivaEDbF"
      },
      "source": [
        "\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "    # weight = torch.from_numpy(train_dataset.tgt_class_weights).to(device)  )  ## For class balancing during training\n",
        "\n",
        "def loss_estimator(pred, truth):\n",
        "    \"\"\" Only consider non-zero inputs in the loss; mask needed\n",
        "    pred: batch\n",
        "    \"\"\"\n",
        "    mask = truth.ge(1).type(torch.FloatTensor).to(device)\n",
        "    loss_ = criterion(pred, truth) * mask\n",
        "    return torch.mean(loss_)\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate,\n",
        "                             weight_decay=0)\n",
        "\n",
        "# scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cebSuzeoRc_H"
      },
      "source": [
        "## Run Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jNa5OWK0DpaH"
      },
      "source": [
        "best_loss = float(\"inf\")\n",
        "best_accuracy = 0\n",
        "for epoch in range(num_epochs):\n",
        "\n",
        "    #-------- Training -------------------\n",
        "    model.train()\n",
        "    acc_loss = 0\n",
        "    running_loss = []\n",
        "    if epoch >= teach_force_till: teacher_forcing = 0\n",
        "    else: teacher_forcing = max(0, teacher_forcing - teach_decay_pereph)\n",
        "\n",
        "    for ith, (src, tgt, src_sz) in enumerate(train_dataloader):\n",
        "\n",
        "        src = src.to(device)\n",
        "        tgt = tgt.to(device)\n",
        "\n",
        "        #--- forward ------\n",
        "        output = model(src = src, tgt = tgt, src_sz =src_sz,\n",
        "                       teacher_forcing_ratio = teacher_forcing)\n",
        "        loss = loss_estimator(output, tgt) / acc_grad\n",
        "        acc_loss += loss\n",
        "\n",
        "        #--- backward ------\n",
        "        loss.backward()\n",
        "        if ( (ith+1) % acc_grad == 0):\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            print('epoch[{}/{}], MiniBatch-{} loss:{:.4f}'\n",
        "                .format(epoch+1, num_epochs, (ith+1)//acc_grad, acc_loss.data))\n",
        "            running_loss.append(acc_loss.item())\n",
        "            acc_loss=0\n",
        "            # break\n",
        "\n",
        "    LOG2CSV(running_loss, LOG_PATH+\"trainLoss.csv\")\n",
        "\n",
        "    #--------- Validate ---------------------\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "    val_accuracy = 0\n",
        "    for jth, (v_src, v_tgt, v_src_sz) in enumerate(tqdm(val_dataloader)):\n",
        "        v_src = v_src.to(device)\n",
        "        v_tgt = v_tgt.to(device)\n",
        "        with torch.no_grad():\n",
        "            v_output = model(src = v_src, tgt = v_tgt, src_sz = v_src_sz)\n",
        "            val_loss += loss_estimator(v_output, v_tgt)\n",
        "\n",
        "            val_accuracy += accuracy_score(v_output, v_tgt, tgt_glyph) # in Utils section\n",
        "        # break\n",
        "    val_loss = val_loss / len(val_dataloader)\n",
        "    val_accuracy = val_accuracy / len(val_dataloader)\n",
        "\n",
        "    print('epoch[{}/{}], [-----TEST------] loss:{:.4f}  Accur:{:.4f}'\n",
        "            .format(epoch+1, num_epochs, val_loss.data, val_accuracy.data))\n",
        "    LOG2CSV([val_loss.item(), val_accuracy.item()],\n",
        "                LOG_PATH+\"valLoss.csv\")\n",
        "\n",
        "    #-------- save Checkpoint -------------------\n",
        "    if val_accuracy > best_accuracy:\n",
        "    # if val_loss < best_loss:\n",
        "        print(\"***saving best optimal state [Loss:{} Accur:{}] ***\".format(val_loss.data,val_accuracy.data) )\n",
        "        best_loss = val_loss\n",
        "        best_accuracy = val_accuracy\n",
        "        torch.save(model.state_dict(), WGT_PREFIX+\"_model.pth\")\n",
        "        LOG2CSV([epoch+1, val_loss.item(), val_accuracy.item()],\n",
        "                LOG_PATH+\"bestCheckpoint.csv\")\n",
        "\n",
        "    # LR step\n",
        "    # scheduler.step()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MCfDiXgyPUoJ"
      },
      "source": [
        "# Inference & Evaluation\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SdO4DI8aXh90"
      },
      "source": [
        "## Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YtLtiPSvV18V"
      },
      "source": [
        "###JSON handling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uGUTK0AwVrGf"
      },
      "source": [
        "def save_to_json(path, data_dict):\n",
        "    with open(path ,\"w\", encoding = \"utf-8\") as f:\n",
        "        json.dump(data_dict, f, ensure_ascii=False, indent=4, sort_keys=True,)\n",
        "\n",
        "\n",
        "def toggle_json(read_path, save_prefix=\"\"):\n",
        "    with open(read_path, 'r', encoding = \"utf-8\") as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "    tog_dict = dict()\n",
        "    for d in data.keys():\n",
        "        for v in data[d]:\n",
        "            tog_dict[v] = set()\n",
        "\n",
        "    for d in data.keys():\n",
        "        for v in data[d]:\n",
        "            tog_dict[v].add(d)\n",
        "\n",
        "    for t in tog_dict.keys():\n",
        "        tog_dict[t] = list(tog_dict[t])\n",
        "\n",
        "    save_file = save_prefix+\"/Toggled-\"+ os.path.basename(read_path)\n",
        "    with open(save_file,\"w\", encoding = \"utf-8\") as f:\n",
        "        json.dump(tog_dict, f, ensure_ascii=False, indent=4, sort_keys=True,)\n",
        "\n",
        "    return save_file\n",
        "\n",
        "\n",
        "def get_from_json(path, ret_data = \"key\"):\n",
        "    with open(path, 'r', encoding = \"utf-8\") as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "    if ret_data == \"key\":\n",
        "        out = list(data.keys())\n",
        "    elif ret_data == \"value\":\n",
        "        temp = data.values()\n",
        "        temp = { i for t in temp for i in t }\n",
        "        out = list(temp)\n",
        "    elif ret_data == \"both\":\n",
        "        out = []\n",
        "        for k in data.keys():\n",
        "            for v in data[k]:\n",
        "                out.append([k,v])\n",
        "    return sorted(out)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XO-MBJ6rU6EV"
      },
      "source": [
        "###Inference Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lR7kl3yJhhdX"
      },
      "source": [
        "Reranking routine based on monolingual vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fr36S5Cfhd9Y"
      },
      "source": [
        "class VocabSanitizer():\n",
        "    '''\n",
        "    Sanitize topK vocab prediction using ancillary vocab list\n",
        "    by reranking or removing etc\n",
        "    '''\n",
        "    def __init__(self, data_file):\n",
        "        '''\n",
        "        data_file: path to file conatining vocabulary list\n",
        "        '''\n",
        "        extension = os.path.splitext(data_file)[-1]\n",
        "        if extension == \".json\":\n",
        "            self.vocab_set  = set( json.load(open(data_file)) )\n",
        "        elif extension == \".csv\":\n",
        "            self.vocab_df = pd.read_csv(data_file).set_index('WORD')\n",
        "            self.vocab_set = set( self.vocab_df.index )\n",
        "        else:\n",
        "            print(\"Only Json/CSV file extension supported\")\n",
        "\n",
        "\n",
        "    def remove_astray(self, word_list):\n",
        "        '''Remove words that are not present in vocabulary\n",
        "        '''\n",
        "        new_list = []\n",
        "        for v in word_list:\n",
        "            if v in self.vocab_set:\n",
        "                new_list.append(v)\n",
        "        if new_list == []:\n",
        "            return word_list.copy()\n",
        "            # return [\" \"]\n",
        "        return new_list\n",
        "\n",
        "    def reposition(self, word_list):\n",
        "        '''Reorder Words in list\n",
        "        '''\n",
        "        new_list = []\n",
        "        temp_ = word_list.copy()\n",
        "        for v in word_list:\n",
        "            if v in self.vocab_set:\n",
        "                new_list.append(v)\n",
        "                temp_.remove(v)\n",
        "        new_list.extend(temp_)\n",
        "\n",
        "        return new_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DwoM0N9rhe_c"
      },
      "source": [
        "Inference runner"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PcexjrRlR8Rf"
      },
      "source": [
        "def inferencer(word, topk = 10):\n",
        "    in_vec = torch.from_numpy(src_glyph.word2xlitvec(word)).to(device)\n",
        "    ## change to active or passive beam\n",
        "    p_out_list = model.active_beam_inference(in_vec, beam_width = topk)\n",
        "    p_result = [ tgt_glyph.xlitvec2word(out.cpu().numpy()) for out in p_out_list]\n",
        "\n",
        "    result = p_result\n",
        "    # result = voc_sanitize.reposition(p_result) ## Uncomment for repositioning\n",
        "    \n",
        "    return result\n",
        "\n",
        "def inference_looper(in_words, topk = 3):\n",
        "    out_dict = {}\n",
        "    for i in tqdm(in_words):\n",
        "        out_dict[i] = inferencer(i, topk=topk)\n",
        "    return out_dict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UNFe5RpLVCNj"
      },
      "source": [
        "##Inferencing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QjMkNDjDLCOo"
      },
      "source": [
        "device = 'cpu'\n",
        "TEST_FILES = [ \"checkup-valid.json\", \"checkup-test.json\", ]\n",
        "WGT_PATH = INST_NAME+\"/weights/\"+INST_NAME+\"_model.pth\"\n",
        "\n",
        "SAVE_DIR = LOG_PATH + \"/acc_log/\"\n",
        "if not os.path.exists(SAVE_DIR): os.makedirs(SAVE_DIR)\n",
        "\n",
        "src_glyph = GlyphStrawboss(english_lower_script) \n",
        "tgt_glyph = GlyphStrawboss(devanagari_script)\n",
        "# voc_sanitize = VocabSanitizer(\"checkup_words_sorted.json\") #Monolingual based topK sorting"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h5YFsfB3bLtn"
      },
      "source": [
        "#Loading Accuracy Computing script\n",
        "!wget https://raw.githubusercontent.com/AI4Bharat/IndianNLP-Transliteration/jgeob-dev/tools/accuracy_reporter/accuracy_news.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-MH09odiVHLc"
      },
      "source": [
        "for fi in TEST_FILES:\n",
        "    tfi =  toggle_json(fi, save_prefix=SAVE_DIR)\n",
        "    words = get_from_json(tfi, \"key\")\n",
        "    out_dict = inference_looper(words, topk = 10)\n",
        "\n",
        "    pred_path = os.path.join(SAVE_DIR, \"pred_\"+os.path.basename(fi) )\n",
        "    save_to_json(pred_path, out_dict)\n",
        "\n",
        "    gt_json = tfi\n",
        "    pred_json = pred_path\n",
        "    save_prefix = os.path.join(SAVE_DIR, os.path.basename(fi).replace(\".json\", \"\"))\n",
        "\n",
        "    for topk in [10, 5, 3, 2, 1]:\n",
        "        ## GT json file passed to below script must be in { En(input): [NativeLang (predict)] } format\n",
        "        run_accuracy_news = \"( echo {} && python accuracy_news.py --gt-json {} --pred-json {} --topk {} --save-output-csv {}_top{}-scores.csv ) | tee -a {}/Summary.txt\".format(\n",
        "                        os.path.basename(fi),\n",
        "                        gt_json, pred_json, topk,\n",
        "                        save_prefix, topk, SAVE_DIR )\n",
        "\n",
        "        os.system(run_accuracy_news)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3loJmEdTkaMH"
      },
      "source": [
        "#Zip"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4tUBGDF-gTCw"
      },
      "source": [
        "# Compress Logs anad Model for Download\n",
        "!zip -r {INST_NAME}.zip {INST_NAME}"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}